---
layout: post
title:  "저렴한 비용으로 높은 프레임 속도의 딥 러닝"
date:   2022-08-29 09:13:47 +0900
categories: Kendryte-K210
comments: true
tags: [Kendryte, K210]
---

## 우리의 접근 방식입니다.

![image](/assets/images/dl/image01.png){: width="50%" height="50%"}

감지 기능이 있는 프레임

센서가 거리의 이미지를 캡처하고 있습니다. 각 이미지 프레임에서 컨볼루션 신경망( YOLO 라고 함 )은 해당 프레임의 객체를 감지합니다. 다음으로, 탐지를 이전에 탐지한 알려진 개체와 연결합니다. 이것을 '객체 추적'이라고 합니다. 마지막으로 물체를 세는지 여부를 결정합니다(주차된 오토바이는 세지 않아도 됨). 결과가 클라우드에 업로드됩니다.

![image](/assets/images/dl/image02.png)

우리의 파이프라인. YOLO는 객체 감지 컨볼루션 신경망입니다. 추적은 개체의 궤적을 형성하는 여러 탐지를 연결합니다. YOLO는 일부 가양성(고스트 개체)을 생성하기 때문에 모든 개체가 계산되는 것은 아닙니다. 마지막으로 카운트가 클라우드에 업로드됩니다.

yolo3의 변형인 객체 감지를 위해 신경망을 실행하는 데 처음에는 프레임당 134밀리초가 걸렸습니다. 긴 시간 범위 때문에 빠르게 움직이는 물체에 대한 추적 성능이 좋지 않았습니다. 추적 알고리즘을 개선하는 방법에 대해 몇 달 동안 연구하는 대신 프레임 속도를 높일 수 있는지 궁금했습니다.

## 동시성 향상

코드의 첫 번째 버전에 대한 빠른 프로파일링 세션의 결과는 다음과 같습니다.

![image](/assets/images/dl/image03.png)

"사진 촬영" 단계가 표시되면 카메라가 사진 촬영을 마칠 때까지 기다리는 것으로 프로세스가 시작되고 CPU에서 약간의 이미지 처리가 수행됩니다. 그 후 YOLO 추론은 KPU에 의해 부분적으로 수행되지만 CPU도 일부(비가속) 부분을 수행해야 합니다. 이 코드는 완전히 순차적입니다. cpu, 카메라 및 kpu는 모두 대기하고 아무 것도 하지 않는 데 상당한 시간을 소비합니다. 이를 수행하는 보다 효율적이고 동시적인 방법이 있어야 합니다.

프레임 속도를 향상시키는 첫 번째이자 가장 쉬운 방법은 대기 시간을 없애는 것입니다. 대신 카메라가 여전히 작업을 수행하는 동안 이전 프레임으로 작업합니다. 이런 식으로 CPU와 카메라 작업이 겹칠 수 있습니다. 물론 여기에는 추가(이미 부족한) 메모리가 필요합니다. 새 이미지가 들어오는 동안 마지막 이미지를 저장하려면 147Ko의 버퍼가 필요합니다. 그러나 이미 41밀리초를 확보했습니다.

![image](/assets/images/dl/image04.png)

예리한 독자는 YOLO 블록과 "그림" 블록 사이에 간격이 있음을 알 수 있습니다. 이것은 설명을 위한 것입니다. 실제로 카메라와 "그림" 블록은 비동기식이므로 언제든지 발생할 수 있습니다(실제로는 순서가 다를 수 있음). YOLO는 단순히 최근에 찍은 사진을 선택합니다.

이제 카메라를 제거했으므로 다른 리소스를 살펴보겠습니다. 분명히 우리는 CPU의 거의 90%를 사용하고 있습니다. 편리하게도 그것은 우리가 처리할 수 있는 두 번째 코어를 고려하지 않은 것입니다. YOLO 모델을 실행하는 작업을 두 번째 코어로 옮기면 버퍼링에 147코가 추가로 소요되지만 이번에는 10밀리초만 보충합니다.

![image](/assets/images/dl/image05.png)

계속하기 전에 두 코어 사이에 엄청난 워크로드 불균형이 있는 것은 거의 불공평해 보입니다. 첫 번째 블록은 사진을 kpu 전용 메모리에 간단하게 업로드하는 것입니다. 첫 번째 CPU에 다시 넣을 때의 유일한 문제는 "kpu가 사용 중일 때 업로드가 금지됨"입니다. 분명히 우리는 칩의 사양에 좌우됩니다. 그러나 숨겨진 방법이 있습니다. 바로 메모리 액세스입니다. 이것은 CPU와 독립적으로 메인 시스템 메모리에 대한 액세스를 제공하는 기능입니다.

![image](/assets/images/dl/image06.png)

이 해킹은 장치의 사양을 따르지 않지만 명백한 단점은 없습니다. 메모리를 소모하지 않으며 영광스러운 13.33fps에 도달할 수 있습니다.

이 시점에서 다음 단계는 kpu와 cpu 2가 수행한 작업을 겹칠 것이라고 예상할 수 있습니다. 그러나 70밀리초에서 회색 선을 발견하셨습니까? 맞습니다. YOLO 끝에 늦은 kpu 작업이 있습니다. 또한 중립 네트워크 가속기가 출력 다운로드에 의해 지속적으로 중단되기 때문에 실제 워크플로는 약간 다르게 보입니다.

![image](/assets/images/dl/image07.png)

## 사자굴에 뛰어들다

더 정밀한 벤치마크는 몇 가지 흥미로운 사실을 보여줍니다.

* 가장 느린 작업은 이미지 업스케일링입니다. 이렇게 하면 채널 크기가 정확히 두 배가 됩니다.
* 기본적으로 벡터 연결인 작업의 경우 채널 연결에는 오랜 시간이 걸립니다.
* 일부 (역)양자화 작업은 동일한 데이터에 연결됩니다. 어쩌면 우리는 그것들을 집계할 수 있습니다.

![image](/assets/images/dl/image08.png)

우리가 할 수 있는 첫 번째 일은 2배만큼 업스케일링을 위한 특수 경로를 추가하는 것입니다. 표준 구현은 (쌍선형) 보간을 수행하는 일반적인 방법을 제공하지만 여기에는 비용이 많이 드는 부동 소수점 연산이 포함됩니다. 많은 노력 없이 이 경로는 크기 조정을 1밀리초로 줄입니다.

다음으로 채널 연결 작업을 제거할 수 있습니다. 메모리에서 데이터는 채널별로 정렬됩니다. 이는 이전 작업이 출력을 올바른 주소에 저장하는 경우 연결을 위해 할 일이 없음을 의미합니다.

마지막으로 부동 소수점 값을 8비트 부동 소수점으로 변환(양자화)하거나 그 반대로(역양자화)하는 작업을 볼 수 있습니다. 이러한 작업은 일부 계수와 함께 사소한 선형 매핑을 적용합니다. 우리 모델에서는 두 매핑이 서로를 취소하는 경우가 있습니다. 따라서 두 작업을 모두 제거할 수 있습니다.

![image](/assets/images/dl/image09.png)

이러한 모든 중복 작업을 제거하는 좋은 부작용은 모델을 실행하기 위한 메모리 공간을 1.1Mo 감소한다는 것입니다.

## 결과: 초당 30프레임

약간의 동시성과 다양한 모델 최적화 덕분에 우리는 프레임 속도를 거의 4배로 늘리고 전체 메모리 공간을 850k로 줄였습니다(이는 RAM 용량의 14%를 나타냄).

![image](/assets/images/dl/image10.png)

최종 데이터 흐름입니다. 프레임당 33밀리초라는 카메라가 우리에게 부과하는 엄격한 제한에 매우 가깝습니다. 대기 시간은 90밀리초로 줄어들었지만 파이프라이닝을 통해 35밀리초 처리량에 도달할 수 있었습니다.

요약하면 $10-$15 칩이라도 실시간으로 최신 심층 신경망을 실행할 수 있으며 결과적으로 추적 문제를 해결할 수 있습니다. 매우 높은 프레임 속도에 도달하는 것 외에도 이러한 종류의 최적화를 통해 필요한 경우 더 큰 모델 네트워크를 사용할 수 있습니다. 정확히 플러그 앤 플레이는 아니지만 시스템 세부 정보를 살펴보는 것은 효과가 있습니다.
